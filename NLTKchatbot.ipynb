{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTKchatbot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsY505C4363jYNk9IhK73Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowshaba/NLTK/blob/main/NLTKchatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-JUvSU1aEpy",
        "outputId": "5dd7777b-dc9f-4d6c-d067-8251e6e06290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "import nltk # natural language tool kits\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "WEVbRLfTa94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(json_file='/starwarsintents.json') :\n",
        "    import json\n",
        "    with open(json_file) as f:\n",
        "        dataset = json.load(f)\n",
        "    \n",
        "    return dataset\n",
        "#daa = load_data()\n",
        "#for i in daa:\n",
        "  #print(i['tag'])\n",
        "  #for r in i['responses']:\n",
        "     # print(r)\n",
        "  \n",
        "    \n",
        "      \n",
        "  \n",
        "  \n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "N59ghe9szja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_words(dataset) :        \n",
        "    words = []\n",
        "    intents_x = []\n",
        "    for intent in dataset:\n",
        "        for p in intent['patterns']+intent['responses'] : # consider both Q & A\n",
        "            #tokens = nltk.word_tokenize(p)\n",
        "            tokens = nltk.regexp_tokenize(p, \"[\\w']+\") # can remove ?\n",
        "            words.extend(tokens)\n",
        "            intents_x.append(tokens)\n",
        "\n",
        "    intent_tags = []\n",
        "    intents_y = []\n",
        "    for intent in dataset:\n",
        "        for p in intent['patterns']+intent['responses'] : # consider both Q & A\n",
        "            intents_y.append(intent[\"tag\"])\n",
        "        if intent['tag'] not in intent_tags:\n",
        "            intent_tags.append(intent['tag'])\n",
        "            \n",
        "    intent_tags = sorted(intent_tags)\n",
        "\n",
        "    #print(\"\\nwords=\")        \n",
        "    #print(words)   \n",
        "    #print(\"\\nintents_x=\")  \n",
        "    #print(intents_x) \n",
        "    #print(\"\\nintents_y=\")   \n",
        "    print(intents_y)\n",
        "    #print(\"\\nintent_tags=\")   \n",
        "    print(intent_tags)\n",
        "\n",
        "    return (words, intents_x, intents_y, intent_tags)\n",
        "dataset = load_data()\n",
        "t=tokenize_words(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTaHVbrt0kVa",
        "outputId": "16a654ff-1caf-4424-86d0-0c9186da7859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'alive', 'alive', 'alive', 'alive', 'alive', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'stories', 'stories', 'stories', 'stories', 'stories']\n",
            "['Menu', 'about me', 'alive', 'bounti hounter', 'creator', 'funny', 'goodbye', 'greeting', 'hepl', 'jedi', 'mission', 'myself', 'sith', 'stories', 'tasks', 'thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(words) :\n",
        "    \n",
        "    \n",
        "    stemmer = nltk.stem.LancasterStemmer()\n",
        "    words_tmp = []\n",
        "    for w in words :\n",
        "        #if w != '?' : words_tmp.append(stemmer.stem(w.lower()))\n",
        "        words_tmp.append(stemmer.stem(w.lower()))\n",
        "            \n",
        "    root_words = sorted(set(words_tmp)) # set() will remove duplications\n",
        "\n",
        "    #print(\"\\nAfter stemming, root words=\") \n",
        "    #print(root_words)    \n",
        "    \n",
        "    return root_words\n",
        "#d=load_data() \n",
        "#words, intents_x, intents_y, intent_tag =tokenize_words(d)\n",
        "#s=stemming(words)    "
      ],
      "metadata": {
        "id": "nFSwQVzo0nx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_data(root_words, intents_x, intents_y, intent_tags) :\n",
        "\n",
        "    stemmer = nltk.stem.LancasterStemmer()\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    for i in range(len(intents_x)):\n",
        "        #for x \n",
        "        bag_of_words = []\n",
        "        words_tmp = []\n",
        "        #print(intents_x[i])\n",
        "        for w in intents_x[i] :\n",
        "            #print(w)\n",
        "            words_tmp.append(stemmer.stem(w.lower()))        \n",
        "            \n",
        "        for w in root_words:\n",
        "            found = 1 if w in words_tmp else 0\n",
        "            bag_of_words.append(found)\n",
        "            \n",
        "        training.append(bag_of_words)\n",
        "        #print(bag_of_words)\n",
        "\n",
        "        #for y\n",
        "        output_row = [0] * len(intent_tags)\n",
        "        #print(output_row)\n",
        "        #print(output_row[intent_tags.index(intents_y[i])])\n",
        "        output_row[intent_tags.index(intents_y[i])] = 1\n",
        "        \n",
        "        output.append(output_row)\n",
        "        #print(output)\n",
        "\n",
        "    train_x = numpy.array(training)\n",
        "    train_y = numpy.array(output)   \n",
        "    \n",
        "    print(\"\\n\")\n",
        "    print(train_x.shape)\n",
        "    print(train_x)\n",
        "    print(train_y.shape)\n",
        "    print(train_y)\n",
        "    \n",
        "    return (train_x, train_y)\n",
        "#d=load_data() \n",
        "#words, intents_x, intents_y, intent_tag =tokenize_words(d)\n",
        "#s=stemming(words)   \n",
        "#f= get_train_data(s, intents_x, intents_y, intent_tag)"
      ],
      "metadata": {
        "id": "GcKr1duF0yqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(train_x, train_y) :\n",
        "    epochs = 1000\n",
        "    batch_size = 32  \n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(batch_size, input_shape=(len(train_x[0]),))) # input layer\n",
        "    model.add(tf.keras.layers.Dense(batch_size)) # hidden layer 1\n",
        "    model.add(tf.keras.layers.Dense(batch_size)) # hidden layer 2\n",
        "    model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax')) # output layer\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Evaluate accuracy of the model '''\n",
        "    model.summary() # for debugging\n",
        "    test_loss, test_acc = model.evaluate(train_x, train_y, verbose=0)\n",
        "    print(\"\\nTested Loss:{} Acc:{}\".format(test_loss, test_acc))\n",
        "    \n",
        "    return model\n",
        "#d=load_data() \n",
        "#words, intents_x, intents_y, intent_tag =tokenize_words(d)\n",
        "#s=stemming(words)   \n",
        "#f, r= get_train_data(s, intents_x, intents_y, intent_tag)\n",
        "#m = build_model(f, r)  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8YR6Kaw1BRb",
        "outputId": "69be6a67-5b94-44c8-c601-2306973cbcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'alive', 'alive', 'alive', 'alive', 'alive', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'stories', 'stories', 'stories', 'stories', 'stories']\n",
            "['Menu', 'about me', 'alive', 'bounti hounter', 'creator', 'funny', 'goodbye', 'greeting', 'hepl', 'jedi', 'mission', 'myself', 'sith', 'stories', 'tasks', 'thanks']\n",
            "\n",
            "\n",
            "(199, 353)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]]\n",
            "(199, 16)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 32)                11328     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,968\n",
            "Trainable params: 13,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Tested Loss:7.932416338007897e-06 Acc:1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model(dataset, model_file=\"model.chatbot\") :\n",
        "    ''' tokenize '''\n",
        "    words, intents_x, intents_y, intent_tags = tokenize_words(dataset)\n",
        "    \n",
        "    ''' stemming '''\n",
        "    root_words = stemming(words)\n",
        "\n",
        "    '''if os.path.isdir(model_file) or os.path.isfile(model_file):\n",
        "        Load existing model \n",
        "        model = tf.keras.models.load_model(model_file)\n",
        "        print(\"Loading model successfully\")\n",
        "    else :'''\n",
        "        #''' Create a new model '''                 \n",
        "    train_x, train_y = get_train_data(root_words, intents_x, intents_y, intent_tags)\n",
        "    model = build_model(train_x, train_y)\n",
        "        #model.save(model_file)\n",
        "    \n",
        "    return (model, root_words, intent_tags)"
      ],
      "metadata": {
        "id": "cpI2MC2f1JC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Ag0BF4bRgiGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, root_words, intent_tags, dataset):\n",
        "    print(\"Start talking with the bot (type quit to stop)\")\n",
        "    while True:\n",
        "        question = input(\"You: \")\n",
        "        if question.lower() == \"quit\": break\n",
        "            \n",
        "        print(root_words) \n",
        "\n",
        "        ''' tokenize '''\n",
        "        tokens = nltk.regexp_tokenize(question, \"[\\w']+\")\n",
        "        print(tokens)\n",
        "\n",
        "        ''' stemming '''\n",
        "        stemmer = nltk.stem.LancasterStemmer()\n",
        "        tokens_tmp = []\n",
        "        for t in tokens:\n",
        "            tokens_tmp.append(stemmer.stem(t.lower()))\n",
        "        print(tokens_tmp)\n",
        "        \n",
        "        ''' bag of words '''\n",
        "        bag_of_words = []\n",
        "        for w in root_words:\n",
        "            \n",
        "            if w in tokens_tmp :\n",
        "                found = 1\n",
        "                print(w)\n",
        "            else :\n",
        "                found = 0\n",
        "            bag_of_words.append(found)\n",
        "        print(bag_of_words)\n",
        "\n",
        "        ''' model predict to find a tag '''\n",
        "        probilities = model.predict(numpy.array([bag_of_words]))\n",
        "        max_index = numpy.argmax(probilities)\n",
        "        max_probility = probilities[0][max_index]\n",
        "        if max_probility < 0.8 :\n",
        "            print(\"Sorry, I don't know what you mean. (confidence: {})\".format(max_probility))\n",
        "            continue\n",
        "        else :\n",
        "            found_tag = intent_tags[max_index]\n",
        "\n",
        "        ''' randomly choose a response according to the found tag '''\n",
        "        for intent in dataset:\n",
        "            if intent['tag'] == found_tag: \n",
        "                print(\"{} (confidence: {})\".format(numpy.random.choice(intent['responses']), max_probility))\n",
        "\n",
        "\n",
        "'''\n",
        "Start up\n",
        "'''\n",
        "dataset = load_data()\n",
        "model, root_words, intent_tags = prepare_model(dataset)\n",
        "chat(model, root_words, intent_tags, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyaZJ85V1N8H",
        "outputId": "4e04dbb4-6e83-4fc5-c5ad-2964ddf10e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'thanks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'tasks', 'alive', 'alive', 'alive', 'alive', 'alive', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'Menu', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'hepl', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'mission', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'jedi', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'sith', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'bounti hounter', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'funny', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'about me', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'creator', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'myself', 'stories', 'stories', 'stories', 'stories', 'stories']\n",
            "['Menu', 'about me', 'alive', 'bounti hounter', 'creator', 'funny', 'goodbye', 'greeting', 'hepl', 'jedi', 'mission', 'myself', 'sith', 'stories', 'tasks', 'thanks']\n",
            "\n",
            "\n",
            "(199, 353)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]]\n",
            "(199, 16)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                11328     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,968\n",
            "Trainable params: 13,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Tested Loss:7.744910362816881e-06 Acc:1.0\n",
            "Start talking with the bot (type quit to stop)\n",
            "You: hi\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['hi']\n",
            "['hi']\n",
            "hi\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Yes, I am here. (confidence: 0.999992847442627)\n",
            "You: jedi\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['jedi']\n",
            "['jed']\n",
            "jed\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Sorry, I don't know what you mean. (confidence: 0.6801935434341431)\n",
            "You: bounti hounter\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['bounti', 'hounter']\n",
            "['bount', 'hount']\n",
            "bount\n",
            "hount\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "I can do whatever you asks me to do (confidence: 0.8995264172554016)\n",
            "You: story\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['story']\n",
            "['story']\n",
            "story\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Sorry, I don't know what you mean. (confidence: 0.363646924495697)\n",
            "You: tell me a story\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['tell', 'me', 'a', 'story']\n",
            "['tel', 'me', 'a', 'story']\n",
            "a\n",
            "me\n",
            "story\n",
            "tel\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "It would be too long for me to speak. (confidence: 0.9999618530273438)\n",
            "You: who are you?\n",
            "['10', '16', '88', 'a', 'aaly', 'abl', 'about', 'address', 'adob', 'adv', 'again', 'ahsok', 'al', 'am', 'amaz', 'anakin', 'and', 'angry', 'any', 'anyon', 'anyth', 'anytim', 'apprecy', 'ar', 'as', 'asajs', 'ask', 'asl', 'assist', 'at', 'aurr', 'back', 'ban', 'bar', 'bathroom', 'be', 'becam', 'becaus', 'beer', 'bef', 'being', 'bel', 'best', 'bit', 'black', 'bloody', 'bob', 'bon', 'bor', 'bount', 'brav', 'breath', 'but', 'button', 'by', 'bye', 'cad', 'cal', 'can', \"can't\", 'car', 'cas', 'catch', 'cath', 'cent', 'check', 'chewy', 'cho', 'choos', 'ciao', 'cod', 'coff', 'com', 'cost', 'could', 'cre', 'd', 'daddy', 'dang', 'dar', 'dark', 'day', 'deal', 'dear', 'deng', 'detail', 'deto', 'develop', 'did', 'do', 'doe', \"don't\", 'doo', 'doubt', 'drink', 'droid', 'drom', 'dumb', 'durg', 'els', 'embo', 'every', 'everyth', 'fath', 'feat', 'fet', 'fil', 'fisto', 'food', 'for', 'forc', 'fre', 'from', 'funny', 'fuzzy', 'galax', 'galaxy', 'get', 'glad', 'glut', 'going', 'gon', 'good', 'goodby', 'gre', 'had', 'haddock', 'half', 'han', 'happy', 'hav', 'he', 'heard', 'hello', 'help', 'her', 'hey', 'hi', 'his', 'hop', 'hount', 'how', 'howdy', 'hum', 'hyperdr', 'i', \"i'm\", 'ident', 'if', 'ig', 'in', 'intellig', 'is', 'issu', 'it', 'item', 'jango', 'jed', 'jet', 'jin', 'jok', 'jov', 'juic', 'just', 'kenob', 'kept', 'kind', 'kit', 'know', 'koon', 'krrsantan', 'kylo', 'lat', 'least', 'lei', 'let', 'lik', 'list', 'littl', 'long', 'look', 'lookimg', 'lot', 'luk', 'mac', 'machin', 'man', 'mark', 'maul', 'may', 'mayb', 'me', 'meet', 'menu', 'millionair', 'mind', 'mis', 'miss', 'most', 'mr', 'my', 'myself', 'nee', 'nev', 'next', 'nic', 'nnot', 'no', 'not', 'now', 'nub', 'ob', 'of', 'off', 'ok', 'on', 'ooooo', 'op', 'optin', 'or', 'org', 'our', 'partn', 'pay', 'pdf', 'peopl', 'person', 'plagu', 'pleas', 'pleass', 'pleassu', 'plo', 'pocket', 'pref', 'princess', 'problem', 'profil', 'program', 'publ', 'push', 'put', 'qel', 'qui', 'r2', 'ragno', 'rant', 'real', 'rebel', 'recommend', 'ren', 'rerout', 'rev', 'right', 'rock', 'run', 's', 'sabin', 'sal', 'say', 'secur', 'see', 'sel', 'select', 'serv', 'shop', 'sid', 'sidy', 'sing', 'sith', 'skyhop', 'skywalk', 'so', 'solo', 'som', 'someon', 'someth', 'soon', 'sorry', 'sort', 'speak', 'spec', 'stag', 'star', 'story', 'sur', 't', 'tak', 'talk', 'tano', 'tauntaun', 'tea', 'tel', 'text', 'than', 'thank', \"thank's\", 'that', \"that's\", 'the', 'ther', 'they', 'thi', 'thing', 'think', 'til', 'tim', 'tnx', 'to', 'toast', 'today', 'too', 'top', 'tray', 'tre', 'tri', 'trick', 'troubl', 'ul', 'up', 'us', 'vad', 'ventress', 'very', 'visit', 'wait', 'wan', 'was', 'wel', 'went', 'wha', 'what', \"what's\", 'whatev', 'which', 'who', 'whol', 'why', 'wil', 'windu', 'wis', 'with', 'wooky', 'worry', 'would', 'wow', 'wren', 'ya', 'ye', 'yo', 'yod', 'you', 'yourself', 'yub']\n",
            "['who', 'are', 'you']\n",
            "['who', 'ar', 'you']\n",
            "ar\n",
            "who\n",
            "you\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "Sorry I can't tell that in public, maybe you are jedi (confidence: 0.9792121648788452)\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}